{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "892230e2-de81-458d-be12-961969971db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba78c8a-154f-4935-ac4a-54bbfd95d115",
   "metadata": {},
   "source": [
    "## Zastosowania języka Python w analizie ekonomicznej\n",
    "Kacper Staroń, 29601\n",
    "Laboratoria, grupa 2\n",
    "### Temat 3 - Liczenie odległości miedzy słowami na Wikipedii.\n",
    "Na potrzeby zadania zaliczeniowego przyjęto że:\n",
    " - projekt jest przewidziany do wyszukiwania haseł na angielskiej wikipedii i nie uwzględnia deklinacji\n",
    " - na potrzeby liczenia odległości między słowami uwzględniane są jedynie merytoryczne elementy artykułu (paragrafy, sekcje i tabele z selektora CSS #bodyContent), nie całej strony \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "57649eff-52a9-4852-8d42-cf1336a3300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateDistance(startWord, searchWord, searchTables = \"infobox_only\", searchSectionNames = True, searchReferences = False, linksFromTables = \"infobox_only\"):\n",
    "    '''\n",
    "    Główna funkcja\n",
    "    Argumenty:\n",
    "        startWord - wyjściowe słowo, hasło ze stroną na wikipedii\n",
    "        searchWord - słowo dla którego będziemy liczyć odległość od hasła z wikipedii (startWord)\n",
    "        searchTables - przyjmuje wartości:\n",
    "            yes - uwzględnij w wyszukiwaniu słowa we wszystkich tabelach, nie tylko zawarte w paragrafach\n",
    "            infobox_only - uwzględnij w wyszukiwaniu słowa w tabelach klasy \"infobox\" (tabele po prawej stronie artykułu)\n",
    "            no - nie uwzględniaj tabel w wyszukiwaniu słów (każdy string inny niż \"yes\" lub \"infobox\" będzie traktowany jako \"no\")\n",
    "        searchSectionNames - przyjmuje wartości True/False, decyduje czy uwzględnić w wyszukiwaniu słów nazwy Sekcji artykułu\n",
    "        searchReferences - przyjmuje wartości True/False, decyduje czy uwzględnić w wyszukiwaniu słów sekcję źródeł\n",
    "        linksFromTables - przyjmuje wartości:\n",
    "            yes - uwzględnij w wyszukiwaniu linki we wszystkich tabelach, nie tylko zawarte w paragrafach\n",
    "            infobox_only - uwzględnij w wyszukiwaniu linki w tabelach klasy \"infobox\" (tabele po prawej stronie artykułu)\n",
    "            no - nie uwzględniaj tabel w wyszukiwaniu linków (każdy string inny niż \"yes\" lub \"infobox\" będzie traktowany jako \"no\")\n",
    "    '''\n",
    "\n",
    "    if searchTables not in [\"yes\", \"no\", \"infobox_only\"]:\n",
    "        print(\"Invalid searchTables\")\n",
    "        return -1\n",
    "    if linksFromTables not in [\"yes\", \"no\", \"infobox_only\"]:\n",
    "        print(\"Invalid linksFromTables\")\n",
    "        return -1\n",
    "    if not isinstance(searchSectionNames, bool):\n",
    "        print(\"Invalid searchSectionNames\")\n",
    "        return -1\n",
    "    if not isinstance(searchReferences, bool):\n",
    "        print(\"Invalid searchReferences\")\n",
    "        return -1\n",
    "    \n",
    "    try:\n",
    "        startWord = startWord.strip()\n",
    "    except: print(\"Invalid startWord\")\n",
    "    try:\n",
    "        searchWord = searchWord.strip().lower()\n",
    "    except: print(\"invalid searchWord\")\n",
    "\n",
    "    \n",
    "    links = {parseWordToWikipediaLink(startWord) : 1} #linki i ich odległości od startowego hasła trzymamy w słowniku dla ominięcia duplikatów\n",
    "    iter = 0\n",
    "\n",
    "    \n",
    "    if not validateStartWord(startWord): #sprawdzenie, czy dla podanego startWord istnieje strona na wikipedii\n",
    "        return \"No wikipedia page for provided word\" \n",
    "\n",
    "    if startWord.lower() == searchWord: #przyjęto, że odległość startowego słowa od samego siebie wynosi 0\n",
    "        return 0\n",
    "\n",
    "    \n",
    "\n",
    "    while True:\n",
    "        distance = search(iter, links, searchWord, searchTables, searchSectionNames, searchReferences, linksFromTables)\n",
    "        if distance:\n",
    "            break\n",
    "        else:\n",
    "            iter +=1\n",
    "\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2d3649b6-e158-4278-9a61-ab6d6f8dd37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validateStartWord(word):\n",
    "    '''\n",
    "    Funkcja do walidowania czy dla podanego słowa istnieje hasło na wikipedii\n",
    "    '''\n",
    "    resp = requests.get(parseWordToWikipediaLink(word))\n",
    "    page = resp.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    return len(soup.css.select(\".noarticletext\")) == 0 #selektor css dla standardowego obiektu wyświetlanego na stronie bez hasła\n",
    "\n",
    "\n",
    "def parseWordToWikipediaLink(word):\n",
    "    '''\n",
    "    Funkcja do zamiany wyrażeń w języku naturalnym na linki do haseł na wikipedii, obsługuje wyrażenia wielowyrazowe ze spacjami\n",
    "    '''\n",
    "    return \"https://en.wikipedia.org/wiki/\" + word.strip().replace(\" \", \"_\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "876fa1ae-31fd-4304-932f-7f88e22843b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(iter, links, searchWord, searchTables, searchSectionNames, searchReferences, linksFromTables):\n",
    "    '''\n",
    "    Funkcja przeszukująca pojedynczą stronę na wikipedii pod kątem szukanego słowa oraz dodająca do podanego słownika linki do innych artykułów\n",
    "    na wikipedii znalezionych na stronie.\n",
    "    W przypadku znalezienia szukanego słowa, zwraca jego odległość od wyjściowego hasła.\n",
    "    Aktualizuje słownik z linkami w miejscu.\n",
    "    '''\n",
    "    pageKey = list(links.keys())[iter]\n",
    "    resp = requests.get(pageKey)\n",
    "    page = resp.text\n",
    "    soup = BeautifulSoup(page, 'html.parser').css.select(\"#bodyContent\")[0] #zawężenie wszystkich przyszłych wyszukiwań do elementów artykulu, nie całej strony\n",
    "    fullText = getPlainTextFromParagraphs(soup) #dodaje do tymczasowej zmiennej tekst z paragrafów\n",
    "    if searchSectionNames:\n",
    "        fullText += getPlainTextFromSectionNames(soup) #dodaje do tymczasowej zmiennej tekst z nazw Sekcji\n",
    "    if searchReferences:\n",
    "        fullText += getPlainTextFromReferences(soup) #dodaje do tymczasowej zmiennej tekst z sekcji źródeł\n",
    "    if searchTables == \"yes\":\n",
    "        fullText += getPlainTextFromTables(soup) #dodaje do tymczasowej zmiennej tekst z tabel\n",
    "    elif searchTables == \"infobox_only\":\n",
    "        fullText += getPlainTextFromInfoboxOnly(soup) #dodaje do tymczasowej zmiennej tekst z infoboxów\n",
    "\n",
    "    if checkSubstring(searchWord, fullText): #sprawdza, czy w zebranych danych tekstowych ze strony występuje szukane słowo\n",
    "        return links[pageKey] \n",
    "\n",
    "    getLinksFromParagraphs(soup, links, pageKey) #dodaje do słownika linków linki z paragrafów wraz z ich odległościami od słowa wyjściowego\n",
    "    if linksFromTables == \"yes\":\n",
    "        getLinksFromTables(soup, links, pageKey) #dodaje do słownika linków linki z tabel wraz z ich odległościami od słowa wyjściowego\n",
    "    elif linksFromTables == \"infobox_only\":\n",
    "        getLinksFromInfoboxOnly(soup, links, pageKey) #dodaje do słownika linków linki z infoboxów wraz z ich odległościami od słowa wyjściowego\n",
    "\n",
    "    return False\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "64a653fb-d6b5-4186-b866-0d0646b4a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcje do pobierania tekstu ze strony\n",
    "\n",
    "def getPlainTextFromParagraphs(soup):\n",
    "    fullText = \"\"\n",
    "    for i in soup.css.select(\"p\"):\n",
    "        fullText += i.getText()\n",
    "    return fullText\n",
    "\n",
    "def getPlainTextFromSectionNames(soup):\n",
    "    fullText = \"\"\n",
    "    for i in soup.css.select(\"div.mw-heading\"):\n",
    "        fullText += i.getText().replace(\"[edit]\", \"\")\n",
    "    return fullText\n",
    "\n",
    "def getPlainTextFromReferences(soup):\n",
    "    fullText = \"\"\n",
    "    for i in soup.css.select(\"cite\"):\n",
    "        fullText += i.getText()\n",
    "    return fullText\n",
    "\n",
    "def getPlainTextFromTables(soup):\n",
    "    fullText = \"\"\n",
    "    for i in soup.css.select(\"table\"):\n",
    "        fullText += i.getText()\n",
    "    return fullText\n",
    "\n",
    "def getPlainTextFromInfoboxOnly(soup):\n",
    "    fullText = \"\"\n",
    "    for i in soup.css.select(\"table\"):\n",
    "        if i.get(\"class\") is not None and \"infobox\" in i.get(\"class\"):\n",
    "            fullText += i.getText()\n",
    "    return fullText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a58ad3f3-40e2-4514-a880-417636d36149",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funkcje do pobierania linków ze strony\n",
    "\n",
    "def getLinksFromParagraphs(soup, links, pageKey):\n",
    "    for i in soup.css.select(\"p\"):\n",
    "        for j in i.find_all(\"a\"):\n",
    "            if j.get(\"href\").startswith(\"/wiki/\") and \"https://en.wikipedia.org\" + j.get(\"href\") not in links:\n",
    "                    links[\"https://en.wikipedia.org\" + j.get(\"href\")] = links[pageKey] + 1\n",
    "    return links\n",
    "\n",
    "def getLinksFromTables(soup, links, iter):\n",
    "    for i in soup.css.select(\"tables\"):\n",
    "        for j in i.find_all(\"a\"):\n",
    "            if j.get(\"href\").startswith(\"/wiki/\") and \"https://en.wikipedia.org\" + j.get(\"href\") not in links:\n",
    "                    links[\"https://en.wikipedia.org\" + j.get(\"href\")] = links[pageKey] + 1\n",
    "    return links\n",
    "\n",
    "def getLinksFromInfoboxOnly(soup, links, iter):\n",
    "    for i in soup.css.select(\"tables\"):\n",
    "        if i.get(\"class\") is not None and \"infobox\" in i.get(\"class\"):\n",
    "            for j in i.find_all(\"a\"):\n",
    "                if j.get(\"href\").startswith(\"/wiki/\") and \"https://en.wikipedia.org\" + j.get(\"href\") not in links:\n",
    "                        links[\"https://en.wikipedia.org\" + j.get(\"href\")] = links[pageKey] + 1\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c959c538-0405-46cc-97d5-6bf937a7af9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkSubstring(word, fullText):\n",
    "    '''\n",
    "    Funkcja sprawdzająca wystąpienia substringa word w stringu fullText z uwzględnieniem czy słowo word występuje w zbiorze słów fullText w sensie językowym\n",
    "    '''\n",
    "    word = word.lower()\n",
    "    fullText = fullText.lower()\n",
    "    n = len(fullText)\n",
    "    pattern = re.compile(\"[^a-zA-Z0-9]\") #regex do sprawdzania, czy znaki bezpośrednio poprzedzające i następujące po znalezionym substringu są literami lub cyframi\n",
    "\n",
    "    for m in re.finditer(word, fullText):\n",
    "        if ((m.start() == 0) or (m.start() > 0 and pattern.match(fullText[m.start() - 1]))) and ((m.end() == n) or (m.end() < n and pattern.match(fullText[m.end()]))):\n",
    "            return True #warunek \"if(): true; else: false\" ze względu na działanie funkcji re.match() \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba30c2e-c37f-467b-8788-185189f95c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4e437f2-9a45-49f8-9557-ff808569ae8c",
   "metadata": {},
   "source": [
    "### Przykład działania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "32fd9b38-7f54-40dd-b8f6-49c99a5349bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 1\n",
      "process_time: 0.234375\n"
     ]
    }
   ],
   "source": [
    "starttime = time.process_time()\n",
    "dist = calculateDistance(\"Dmitri_Sannikov\", \"russian\")\n",
    "endtime = time.process_time()\n",
    "print(\"Distance: \" + str(dist))\n",
    "print(\"process_time: \" + str(endtime - starttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "99e334fb-a700-49cd-bd64-1785b134d04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: 2\n",
      "process_time: 2.234375\n"
     ]
    }
   ],
   "source": [
    "starttime = time.process_time()\n",
    "dist = calculateDistance(\"Dmitri_Sannikov\", \"soccer\")\n",
    "endtime = time.process_time()\n",
    "print(\"Distance: \" + str(dist))\n",
    "print(\"process_time: \" + str(endtime - starttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8eff0a69-86fb-4ae6-b540-580791acd24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance: No wikipedia page for provided word\n",
      "process_time: 0.109375\n"
     ]
    }
   ],
   "source": [
    "starttime = time.process_time()\n",
    "dist = calculateDistance(\"Dmi_Sann\", \"russian\")\n",
    "endtime = time.process_time()\n",
    "print(\"Distance: \" + str(dist))\n",
    "print(\"process_time: \" + str(endtime - starttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a42097-78e1-4653-b7f0-7fa5d4dea606",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
